{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9b4adee-32ed-41ec-bcbc-e03043d61e37",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf6c7eb-3409-4a34-a9f0-c86e3a48f52f",
   "metadata": {},
   "source": [
    "# Machine Learning Foundation\n",
    "\n",
    "## Course 5, Part e: CNN DEMO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a514949-93e4-4439-a58e-167df573b7f2",
   "metadata": {},
   "source": [
    "## Building a CNN to classify images in the CIFAR-10 Dataset\n",
    "\n",
    "We will work with the CIFAR-10 Dataset.  This is a well-known dataset for image classification, which consists of 60000 32x32 color images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
    "\n",
    "The 10 classes are:\n",
    "\n",
    "<ol start=\"0\">\n",
    "<li> airplane\n",
    "<li>  automobile\n",
    "<li> bird\n",
    "<li>  cat\n",
    "<li> deer\n",
    "<li> dog\n",
    "<li>  frog\n",
    "<li>  horse\n",
    "<li>  ship\n",
    "<li>  truck\n",
    "</ol>\n",
    "\n",
    "For details about CIFAR-10 see:\n",
    "https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "\n",
    "For a compilation of published performance results on CIFAR 10, see:\n",
    "http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html\n",
    "\n",
    "<p style='color: blue'>Note: Training CNNs takes time, you should expect waiting time to be around 10 ~ 30 mins after running a cell that trains a CNN.</p>\n",
    "\n",
    "---\n",
    "\n",
    "### Building Convolutional Neural Nets\n",
    "\n",
    "In this exercise we will build and train our first convolutional neural networks.  In the first part, we walk through the different layers and how they are configured.  In the second part, you will build your own model, train it, and compare the performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4480d8fc-2285-43d8-a997-7bf18b4481be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6c6db09-4c2c-44bc-8729-c137e2340f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# The data, shuffled and split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91c16f06-8fcf-4b30-9eb0-5e37862965b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Each image is a 32 x 32 x 3 numpy array\n",
    "x_train[444].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "193dc886-0db9-47e2-a97b-6e78816add28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Let's look at one of the images\n",
    "\n",
    "print(y_train[444])\n",
    "plt.imshow(x_train[444]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa4c9ce6-1a74-4324-9b5a-da12bbedcb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d50a2044-79cf-4b26-8755-d81e17a15d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now instead of classes described by an integer between 0-9 we have a vector with a 1 in the (Pythonic) 9th position\n",
    "y_train[444]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e22830ed-3e20-4651-b1cb-1af977ab4da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As before, let's make everything float and scale\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b8adcf-5e05-4b99-b9fe-eb3c0eebf957",
   "metadata": {},
   "source": [
    "## Keras Layers for CNNs\n",
    "- Previously we built Neural Networks using primarily the Dense, Activation and Dropout Layers.\n",
    "\n",
    "- Here we will describe how to use some of the CNN-specific layers provided by Keras\n",
    "\n",
    "### Conv2D\n",
    "\n",
    "```python\n",
    "keras.layers.convolutional.Conv2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)\n",
    "```\n",
    "\n",
    "A few parameters explained:\n",
    "- `filters`: the number of filter used per location.  In other words, the depth of the output.\n",
    "- `kernel_size`: an (x,y) tuple giving the height and width of the kernel to be used\n",
    "- `strides`: and (x,y) tuple giving the stride in each dimension.  Default is `(1,1)`\n",
    "- `input_shape`: required only for the first layer\n",
    "\n",
    "Note, the size of the output will be determined by the kernel_size, strides\n",
    "\n",
    "### MaxPooling2D\n",
    "`keras.layers.pooling.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)`\n",
    "\n",
    "- `pool_size`: the (x,y) size of the grid to be pooled.\n",
    "- `strides`: Assumed to be the `pool_size` unless otherwise specified\n",
    "\n",
    "### Flatten\n",
    "Turns its input into a one-dimensional vector (per instance).  Usually used when transitioning between convolutional layers and fully connected layers.\n",
    "\n",
    "---\n",
    "\n",
    "## First CNN\n",
    "Below we will build our first CNN.  For demonstration purposes (so that it will train quickly) it is not very deep and has relatively few parameters.  We use strides of 2 in the first two convolutional layers which quickly reduces the dimensions of the output.  After a MaxPooling layer, we flatten, and then have a single fully connected layer before our final classification layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "208ef451-33c3-4b66-b090-5c3d80d9cce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 16, 16, 32)        2432      \n",
      "                                                                 \n",
      " activation (Activation)     (None, 16, 16, 32)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 6, 6, 32)          25632     \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 6, 6, 32)          0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 3, 3, 32)         0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 3, 3, 32)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 288)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               147968    \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 512)               0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 181,162\n",
      "Trainable params: 181,162\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Let's build a CNN using Keras' Sequential capabilities\n",
    "\n",
    "model_1 = Sequential()\n",
    "\n",
    "\n",
    "## 5x5 convolution with 2x2 stride and 32 filters\n",
    "model_1.add(Conv2D(32, (5, 5), strides = (2,2), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model_1.add(Activation('relu'))\n",
    "\n",
    "## Another 5x5 convolution with 2x2 stride and 32 filters\n",
    "model_1.add(Conv2D(32, (5, 5), strides = (2,2)))\n",
    "model_1.add(Activation('relu'))\n",
    "\n",
    "## 2x2 max pooling reduces to 3 x 3 x 32\n",
    "model_1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_1.add(Dropout(0.25))\n",
    "\n",
    "## Flatten turns 3x3x32 into 288x1\n",
    "model_1.add(Flatten())\n",
    "model_1.add(Dense(512))\n",
    "model_1.add(Activation('relu'))\n",
    "model_1.add(Dropout(0.5))\n",
    "model_1.add(Dense(num_classes))\n",
    "model_1.add(Activation('softmax'))\n",
    "\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25618a91-080a-4072-9ef7-67da0fd1dba9",
   "metadata": {},
   "source": [
    "We still have 181K parameters, even though this is a \"small\" model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a2fac4d-ebe0-4231-9772-b2056ecdd950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1563/1563 [==============================] - 18s 7ms/step - loss: 1.7330 - accuracy: 0.3672 - val_loss: 1.4259 - val_accuracy: 0.4868\n",
      "Epoch 2/15\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 1.4392 - accuracy: 0.4841 - val_loss: 1.2961 - val_accuracy: 0.5334\n",
      "Epoch 3/15\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 1.3352 - accuracy: 0.5232 - val_loss: 1.2397 - val_accuracy: 0.5572\n",
      "Epoch 4/15\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 1.2847 - accuracy: 0.5437 - val_loss: 1.1875 - val_accuracy: 0.5790\n",
      "Epoch 5/15\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.2363 - accuracy: 0.5616 - val_loss: 1.1804 - val_accuracy: 0.5814\n",
      "Epoch 6/15\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 1.2030 - accuracy: 0.5759 - val_loss: 1.1235 - val_accuracy: 0.6062\n",
      "Epoch 7/15\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 1.1771 - accuracy: 0.5862 - val_loss: 1.1242 - val_accuracy: 0.6089\n",
      "Epoch 8/15\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.1628 - accuracy: 0.5928 - val_loss: 1.1126 - val_accuracy: 0.6099\n",
      "Epoch 9/15\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.1502 - accuracy: 0.5982 - val_loss: 1.0876 - val_accuracy: 0.6197\n",
      "Epoch 10/15\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.1366 - accuracy: 0.6036 - val_loss: 1.0863 - val_accuracy: 0.6258\n",
      "Epoch 11/15\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 1.1287 - accuracy: 0.6090 - val_loss: 1.0533 - val_accuracy: 0.6287\n",
      "Epoch 12/15\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 1.1148 - accuracy: 0.6126 - val_loss: 1.0822 - val_accuracy: 0.6199\n",
      "Epoch 13/15\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 1.1111 - accuracy: 0.6160 - val_loss: 1.0881 - val_accuracy: 0.6256\n",
      "Epoch 14/15\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 1.1027 - accuracy: 0.6169 - val_loss: 1.0952 - val_accuracy: 0.6216\n",
      "Epoch 15/15\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 1.1019 - accuracy: 0.6217 - val_loss: 1.0489 - val_accuracy: 0.6476\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c229d83fa0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.RMSprop(lr=0.0005)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model_1.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_1.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=15,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e808a124-84c7-47f7-a6b9-13f47bde7f60",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Our previous model had the structure:\n",
    "\n",
    "Conv -> Conv -> MaxPool -> (Flatten) -> Dense -> Final Classification\n",
    "\n",
    "(with appropriate activation functions and dropouts)\n",
    "\n",
    "1. Build a more complicated model with the following pattern:\n",
    "- Conv -> Conv -> MaxPool -> Conv -> Conv -> MaxPool -> (Flatten) -> Dense -> Final Classification\n",
    "\n",
    "- Use strides of 1 for all convolutional layers.\n",
    "\n",
    "2. How many parameters does your model have?  How does that compare to the previous model?\n",
    "\n",
    "3. Train it for 5 epochs.  What do you notice about the training time, loss and accuracy numbers (on both the training and validation sets)?\n",
    "\n",
    "5. Try different structures and run times, and see how accurate your model can be.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f06f4741-789c-4219-a286-baaa5d36a04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build a CNN using Keras' Sequential capabilities\n",
    "\n",
    "model_2 = Sequential()\n",
    "\n",
    "model_2.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model_2.add(Activation('relu'))\n",
    "model_2.add(Conv2D(32, (3, 3)))\n",
    "model_2.add(Activation('relu'))\n",
    "model_2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_2.add(Dropout(0.25))\n",
    "\n",
    "model_2.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model_2.add(Activation('relu'))\n",
    "model_2.add(Conv2D(64, (3, 3)))\n",
    "model_2.add(Activation('relu'))\n",
    "model_2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_2.add(Dropout(0.25))\n",
    "\n",
    "model_2.add(Flatten())\n",
    "model_2.add(Dense(512))\n",
    "model_2.add(Activation('relu'))\n",
    "model_2.add(Dropout(0.5))\n",
    "model_2.add(Dense(num_classes))\n",
    "model_2.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef06d5ac-e13a-4f34-97c4-521ca9baf9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 32, 32, 32)        0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 30, 30, 32)        9248      \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 30, 30, 32)        0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 15, 15, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 15, 15, 32)        0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 15, 15, 64)        18496     \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 15, 15, 64)        0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 13, 13, 64)        36928     \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 13, 13, 64)        0         \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 6, 6, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 6, 6, 64)          0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 2304)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 512)               1180160   \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 512)               0         \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      " activation_9 (Activation)   (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,250,858\n",
      "Trainable params: 1,250,858\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Check number of parameters\n",
    "\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4018b70b-af1d-47a5-afc3-e07ac5a37323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate RMSprop optimizer\n",
    "opt_2 = keras.optimizers.RMSprop(lr=0.0005)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model_2.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt_2,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "116d40b9-c69e-4778-8d3d-e59a89c3c523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1563/1563 [==============================] - 17s 10ms/step - loss: 1.5955 - accuracy: 0.4216 - val_loss: 1.2144 - val_accuracy: 0.5673\n",
      "Epoch 2/5\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 1.1938 - accuracy: 0.5795 - val_loss: 1.1175 - val_accuracy: 0.6209\n",
      "Epoch 3/5\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.0116 - accuracy: 0.6494 - val_loss: 0.8848 - val_accuracy: 0.6942\n",
      "Epoch 4/5\n",
      "1563/1563 [==============================] - 15s 10ms/step - loss: 0.9157 - accuracy: 0.6828 - val_loss: 0.9480 - val_accuracy: 0.6725\n",
      "Epoch 5/5\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 0.8541 - accuracy: 0.7054 - val_loss: 0.8696 - val_accuracy: 0.7013\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c24425baf0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=5,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4da852-b48b-40ee-9494-971cc022937c",
   "metadata": {},
   "source": [
    "---\n",
    "### Machine Learning Foundation (C) 2020 IBM Corporation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "catboost",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "prev_pub_hash": "da0f7caf9ceec94ee79d87b755236695caf57b021b059786f765a5fe0f9cdaaf"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
